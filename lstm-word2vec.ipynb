{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jorge/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jorge/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/jorge/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Embedding, GlobalMaxPooling1D, Dropout, LSTM,Input,Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from sklearn.metrics import f1_score\n",
    "import feature_builder\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "embeddings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliteo para obtener vectores de train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "train,test = train_test_split(train_df,test_size=0.33,random_state = 17)\n",
    "train.reset_index(inplace=True,drop=True)\n",
    "test.reset_index(inplace=True,drop=True)\n",
    "y_train = train['target'].values\n",
    "y_test = test['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_cnn(df):\n",
    "    processed = feature_builder.process_dataset(df)\n",
    "    return (processed, processed.to_numpy().reshape(processed.shape[0], 1, processed.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of words covered in the embeddings = 0.6691291143396536\n"
     ]
    }
   ],
   "source": [
    "processed, X_train = prepare_for_cnn(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invalid_location_character_count</th>\n",
       "      <th>location_is_place</th>\n",
       "      <th>mean_encode</th>\n",
       "      <th>keyword_length</th>\n",
       "      <th>text_length</th>\n",
       "      <th>location_length</th>\n",
       "      <th>stop_word_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>...</th>\n",
       "      <th>text_embedding_290</th>\n",
       "      <th>text_embedding_291</th>\n",
       "      <th>text_embedding_292</th>\n",
       "      <th>text_embedding_293</th>\n",
       "      <th>text_embedding_294</th>\n",
       "      <th>text_embedding_295</th>\n",
       "      <th>text_embedding_296</th>\n",
       "      <th>text_embedding_297</th>\n",
       "      <th>text_embedding_298</th>\n",
       "      <th>text_embedding_299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.332394</td>\n",
       "      <td>7</td>\n",
       "      <td>137</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095388</td>\n",
       "      <td>0.010420</td>\n",
       "      <td>-0.137559</td>\n",
       "      <td>0.057483</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>-0.003357</td>\n",
       "      <td>0.010531</td>\n",
       "      <td>-0.027733</td>\n",
       "      <td>0.050691</td>\n",
       "      <td>-0.003874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.280751</td>\n",
       "      <td>6</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004451</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>-0.093940</td>\n",
       "      <td>-0.007401</td>\n",
       "      <td>-0.119637</td>\n",
       "      <td>-0.038094</td>\n",
       "      <td>-0.047624</td>\n",
       "      <td>-0.061011</td>\n",
       "      <td>0.035588</td>\n",
       "      <td>-0.002136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>8</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055859</td>\n",
       "      <td>0.084729</td>\n",
       "      <td>-0.073218</td>\n",
       "      <td>0.051086</td>\n",
       "      <td>-0.057727</td>\n",
       "      <td>-0.052509</td>\n",
       "      <td>-0.060059</td>\n",
       "      <td>-0.065416</td>\n",
       "      <td>-0.017188</td>\n",
       "      <td>-0.016446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.557243</td>\n",
       "      <td>6</td>\n",
       "      <td>127</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086775</td>\n",
       "      <td>0.041582</td>\n",
       "      <td>-0.071882</td>\n",
       "      <td>0.044573</td>\n",
       "      <td>-0.020787</td>\n",
       "      <td>-0.062112</td>\n",
       "      <td>0.016305</td>\n",
       "      <td>-0.109480</td>\n",
       "      <td>0.047154</td>\n",
       "      <td>-0.120989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.219078</td>\n",
       "      <td>12</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009979</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>-0.133545</td>\n",
       "      <td>0.085571</td>\n",
       "      <td>-0.168243</td>\n",
       "      <td>-0.097534</td>\n",
       "      <td>-0.000244</td>\n",
       "      <td>-0.144165</td>\n",
       "      <td>0.015339</td>\n",
       "      <td>-0.027222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5095</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.638310</td>\n",
       "      <td>5</td>\n",
       "      <td>105</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043355</td>\n",
       "      <td>-0.133185</td>\n",
       "      <td>-0.119283</td>\n",
       "      <td>-0.045953</td>\n",
       "      <td>-0.127252</td>\n",
       "      <td>-0.017334</td>\n",
       "      <td>-0.079007</td>\n",
       "      <td>-0.119819</td>\n",
       "      <td>-0.020020</td>\n",
       "      <td>0.124390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5096</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.420423</td>\n",
       "      <td>11</td>\n",
       "      <td>137</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049957</td>\n",
       "      <td>0.040430</td>\n",
       "      <td>0.012134</td>\n",
       "      <td>-0.002661</td>\n",
       "      <td>-0.030566</td>\n",
       "      <td>0.025269</td>\n",
       "      <td>0.071167</td>\n",
       "      <td>-0.076440</td>\n",
       "      <td>0.014407</td>\n",
       "      <td>0.018445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.839437</td>\n",
       "      <td>6</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009125</td>\n",
       "      <td>0.053214</td>\n",
       "      <td>-0.100740</td>\n",
       "      <td>0.015198</td>\n",
       "      <td>0.075342</td>\n",
       "      <td>-0.050513</td>\n",
       "      <td>-0.169012</td>\n",
       "      <td>-0.083276</td>\n",
       "      <td>0.094217</td>\n",
       "      <td>-0.002344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5098</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.653208</td>\n",
       "      <td>7</td>\n",
       "      <td>132</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040901</td>\n",
       "      <td>-0.013988</td>\n",
       "      <td>-0.059555</td>\n",
       "      <td>0.041462</td>\n",
       "      <td>-0.039307</td>\n",
       "      <td>0.086788</td>\n",
       "      <td>-0.073860</td>\n",
       "      <td>-0.136898</td>\n",
       "      <td>0.056950</td>\n",
       "      <td>0.062004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5099</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.307355</td>\n",
       "      <td>8</td>\n",
       "      <td>82</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081340</td>\n",
       "      <td>-0.023926</td>\n",
       "      <td>-0.088440</td>\n",
       "      <td>0.033040</td>\n",
       "      <td>-0.007731</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>-0.114543</td>\n",
       "      <td>-0.182220</td>\n",
       "      <td>0.028890</td>\n",
       "      <td>-0.016357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5100 rows Ã— 332 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      invalid_location_character_count  location_is_place  mean_encode  \\\n",
       "0                                    0                  0     0.332394   \n",
       "1                                    0                  0     0.280751   \n",
       "2                                    0                  0     0.600000   \n",
       "3                                    4                  0     0.557243   \n",
       "4                                    0                  0     0.219078   \n",
       "...                                ...                ...          ...   \n",
       "5095                                 0                  1     0.638310   \n",
       "5096                                 0                  1     0.420423   \n",
       "5097                                 0                  0     0.839437   \n",
       "5098                                 0                  0     0.653208   \n",
       "5099                                10                  0     0.307355   \n",
       "\n",
       "      keyword_length  text_length  location_length  stop_word_count  \\\n",
       "0                  7          137                9                8   \n",
       "1                  6          117                0               12   \n",
       "2                  8           80                0               10   \n",
       "3                  6          127               25                5   \n",
       "4                 12           44                0                3   \n",
       "...              ...          ...              ...              ...   \n",
       "5095               5          105               17                3   \n",
       "5096              11          137               11                2   \n",
       "5097               6          105                0                2   \n",
       "5098               7          132               13                3   \n",
       "5099               8           82               24                2   \n",
       "\n",
       "      punctuation_count  hashtag_count  mention_count  ...  \\\n",
       "0                     2              0              0  ...   \n",
       "1                     0              0              0  ...   \n",
       "2                     2              0              0  ...   \n",
       "3                    15              0              2  ...   \n",
       "4                     8              1              0  ...   \n",
       "...                 ...            ...            ...  ...   \n",
       "5095                  5              0              0  ...   \n",
       "5096                 11              0              0  ...   \n",
       "5097                  5              0              0  ...   \n",
       "5098                  1              0              0  ...   \n",
       "5099                 17              0              0  ...   \n",
       "\n",
       "      text_embedding_290  text_embedding_291  text_embedding_292  \\\n",
       "0              -0.095388            0.010420           -0.137559   \n",
       "1              -0.004451            0.003784           -0.093940   \n",
       "2               0.055859            0.084729           -0.073218   \n",
       "3              -0.086775            0.041582           -0.071882   \n",
       "4               0.009979            0.008453           -0.133545   \n",
       "...                  ...                 ...                 ...   \n",
       "5095           -0.043355           -0.133185           -0.119283   \n",
       "5096           -0.049957            0.040430            0.012134   \n",
       "5097            0.009125            0.053214           -0.100740   \n",
       "5098            0.040901           -0.013988           -0.059555   \n",
       "5099           -0.081340           -0.023926           -0.088440   \n",
       "\n",
       "      text_embedding_293  text_embedding_294  text_embedding_295  \\\n",
       "0               0.057483            0.004149           -0.003357   \n",
       "1              -0.007401           -0.119637           -0.038094   \n",
       "2               0.051086           -0.057727           -0.052509   \n",
       "3               0.044573           -0.020787           -0.062112   \n",
       "4               0.085571           -0.168243           -0.097534   \n",
       "...                  ...                 ...                 ...   \n",
       "5095           -0.045953           -0.127252           -0.017334   \n",
       "5096           -0.002661           -0.030566            0.025269   \n",
       "5097            0.015198            0.075342           -0.050513   \n",
       "5098            0.041462           -0.039307            0.086788   \n",
       "5099            0.033040           -0.007731            0.002116   \n",
       "\n",
       "      text_embedding_296  text_embedding_297  text_embedding_298  \\\n",
       "0               0.010531           -0.027733            0.050691   \n",
       "1              -0.047624           -0.061011            0.035588   \n",
       "2              -0.060059           -0.065416           -0.017188   \n",
       "3               0.016305           -0.109480            0.047154   \n",
       "4              -0.000244           -0.144165            0.015339   \n",
       "...                  ...                 ...                 ...   \n",
       "5095           -0.079007           -0.119819           -0.020020   \n",
       "5096            0.071167           -0.076440            0.014407   \n",
       "5097           -0.169012           -0.083276            0.094217   \n",
       "5098           -0.073860           -0.136898            0.056950   \n",
       "5099           -0.114543           -0.182220            0.028890   \n",
       "\n",
       "      text_embedding_299  \n",
       "0              -0.003874  \n",
       "1              -0.002136  \n",
       "2              -0.016446  \n",
       "3              -0.120989  \n",
       "4              -0.027222  \n",
       "...                  ...  \n",
       "5095            0.124390  \n",
       "5096            0.018445  \n",
       "5097           -0.002344  \n",
       "5098            0.062004  \n",
       "5099           -0.016357  \n",
       "\n",
       "[5100 rows x 332 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    model = Sequential([\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 128)               203264    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 211,585\n",
      "Trainable params: 211,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "128/128 [==============================] - 2s 17ms/step - loss: 0.6549 - accuracy: 0.5809 - val_loss: 0.6661 - val_accuracy: 0.5265\n",
      "Epoch 2/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6289 - accuracy: 0.5824 - val_loss: 0.6495 - val_accuracy: 0.5363\n",
      "Epoch 3/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.6105 - accuracy: 0.6034 - val_loss: 0.6348 - val_accuracy: 0.5833\n",
      "Epoch 4/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.5926 - accuracy: 0.6382 - val_loss: 0.6330 - val_accuracy: 0.5775\n",
      "Epoch 5/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.5779 - accuracy: 0.6645 - val_loss: 0.6225 - val_accuracy: 0.5922\n",
      "Epoch 6/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.5612 - accuracy: 0.6821 - val_loss: 0.5937 - val_accuracy: 0.6735\n",
      "Epoch 7/100\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.5450 - accuracy: 0.7044 - val_loss: 0.5823 - val_accuracy: 0.6843\n",
      "Epoch 8/100\n",
      "128/128 [==============================] - 2s 13ms/step - loss: 0.5301 - accuracy: 0.7233 - val_loss: 0.5699 - val_accuracy: 0.6863\n",
      "Epoch 9/100\n",
      "128/128 [==============================] - 2s 14ms/step - loss: 0.5141 - accuracy: 0.7431 - val_loss: 0.5593 - val_accuracy: 0.6873\n",
      "Epoch 10/100\n",
      "128/128 [==============================] - 2s 15ms/step - loss: 0.4970 - accuracy: 0.7493 - val_loss: 0.5425 - val_accuracy: 0.7235\n",
      "Epoch 11/100\n",
      "128/128 [==============================] - 2s 15ms/step - loss: 0.4837 - accuracy: 0.7650 - val_loss: 0.5489 - val_accuracy: 0.6922\n",
      "Epoch 12/100\n",
      "128/128 [==============================] - 2s 12ms/step - loss: 0.4704 - accuracy: 0.7711 - val_loss: 0.5432 - val_accuracy: 0.6971\n",
      "Epoch 13/100\n",
      "128/128 [==============================] - 2s 16ms/step - loss: 0.4609 - accuracy: 0.7794 - val_loss: 0.5238 - val_accuracy: 0.7373\n",
      "Epoch 14/100\n",
      "128/128 [==============================] - 2s 17ms/step - loss: 0.4508 - accuracy: 0.7826 - val_loss: 0.5175 - val_accuracy: 0.7412\n",
      "Epoch 15/100\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.4427 - accuracy: 0.7926 - val_loss: 0.5169 - val_accuracy: 0.7402\n",
      "Epoch 16/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.4358 - accuracy: 0.7926 - val_loss: 0.5042 - val_accuracy: 0.7657\n",
      "Epoch 17/100\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.4276 - accuracy: 0.7971 - val_loss: 0.5050 - val_accuracy: 0.7490\n",
      "Epoch 18/100\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.4202 - accuracy: 0.8054 - val_loss: 0.5209 - val_accuracy: 0.7324\n",
      "Epoch 19/100\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.4144 - accuracy: 0.8076 - val_loss: 0.5084 - val_accuracy: 0.7471\n",
      "Epoch 20/100\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.4092 - accuracy: 0.8113 - val_loss: 0.4969 - val_accuracy: 0.7608\n",
      "Epoch 21/100\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.4024 - accuracy: 0.8179 - val_loss: 0.4937 - val_accuracy: 0.7745\n",
      "Epoch 22/100\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.4008 - accuracy: 0.8162 - val_loss: 0.4976 - val_accuracy: 0.7608\n",
      "Epoch 23/100\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3945 - accuracy: 0.8189 - val_loss: 0.4966 - val_accuracy: 0.7755\n",
      "Epoch 24/100\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3913 - accuracy: 0.8218 - val_loss: 0.4928 - val_accuracy: 0.7706\n",
      "Epoch 25/100\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 0.3849 - accuracy: 0.8255 - val_loss: 0.4930 - val_accuracy: 0.7765\n",
      "Epoch 26/100\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3850 - accuracy: 0.8250 - val_loss: 0.4904 - val_accuracy: 0.7765\n",
      "Epoch 27/100\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3775 - accuracy: 0.8328 - val_loss: 0.4909 - val_accuracy: 0.7765\n",
      "Epoch 28/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.3746 - accuracy: 0.8304 - val_loss: 0.5046 - val_accuracy: 0.7686\n",
      "Epoch 29/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.3722 - accuracy: 0.8314 - val_loss: 0.4971 - val_accuracy: 0.7745\n",
      "Epoch 30/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.3686 - accuracy: 0.8375 - val_loss: 0.4919 - val_accuracy: 0.7765\n",
      "Epoch 31/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.3664 - accuracy: 0.8355 - val_loss: 0.4874 - val_accuracy: 0.7873\n",
      "Epoch 32/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.3656 - accuracy: 0.8395 - val_loss: 0.4997 - val_accuracy: 0.7794\n",
      "Epoch 33/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.3584 - accuracy: 0.8412 - val_loss: 0.4931 - val_accuracy: 0.7843\n",
      "Epoch 34/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.3581 - accuracy: 0.8395 - val_loss: 0.4901 - val_accuracy: 0.7784\n",
      "Epoch 35/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.3528 - accuracy: 0.8441 - val_loss: 0.5017 - val_accuracy: 0.7775\n",
      "Epoch 36/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.3505 - accuracy: 0.8451 - val_loss: 0.4910 - val_accuracy: 0.7804\n",
      "Epoch 37/100\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 0.3462 - accuracy: 0.8446 - val_loss: 0.4985 - val_accuracy: 0.7843\n",
      "Epoch 38/100\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3475 - accuracy: 0.8473 - val_loss: 0.5035 - val_accuracy: 0.7794\n",
      "Epoch 39/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.3415 - accuracy: 0.8439 - val_loss: 0.4970 - val_accuracy: 0.7804\n",
      "Epoch 40/100\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.3414 - accuracy: 0.8475 - val_loss: 0.4987 - val_accuracy: 0.7892\n",
      "Epoch 41/100\n",
      "128/128 [==============================] - 1s 12ms/step - loss: 0.3379 - accuracy: 0.8507 - val_loss: 0.5160 - val_accuracy: 0.7725\n",
      "Epoch 42/100\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3362 - accuracy: 0.8493 - val_loss: 0.5000 - val_accuracy: 0.7863\n",
      "Epoch 43/100\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3317 - accuracy: 0.8569 - val_loss: 0.5012 - val_accuracy: 0.7873\n",
      "Epoch 44/100\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3337 - accuracy: 0.8471 - val_loss: 0.5006 - val_accuracy: 0.7863\n",
      "Epoch 45/100\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3271 - accuracy: 0.8551 - val_loss: 0.5122 - val_accuracy: 0.7833\n",
      "Epoch 46/100\n",
      "128/128 [==============================] - 2s 12ms/step - loss: 0.3267 - accuracy: 0.8525 - val_loss: 0.5215 - val_accuracy: 0.7755\n",
      "Epoch 47/100\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3220 - accuracy: 0.8556 - val_loss: 0.5109 - val_accuracy: 0.7824\n",
      "Epoch 48/100\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3203 - accuracy: 0.8605 - val_loss: 0.5046 - val_accuracy: 0.7784\n",
      "Epoch 49/100\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.3191 - accuracy: 0.8596 - val_loss: 0.4996 - val_accuracy: 0.7902\n",
      "Epoch 50/100\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3163 - accuracy: 0.8581 - val_loss: 0.5027 - val_accuracy: 0.7902\n",
      "Epoch 51/100\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3131 - accuracy: 0.8593 - val_loss: 0.5056 - val_accuracy: 0.7843\n",
      "Epoch 52/100\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3149 - accuracy: 0.8610 - val_loss: 0.5071 - val_accuracy: 0.7853\n",
      "Epoch 53/100\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3112 - accuracy: 0.8625 - val_loss: 0.5057 - val_accuracy: 0.7853\n",
      "Epoch 54/100\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 0.3094 - accuracy: 0.8652 - val_loss: 0.5127 - val_accuracy: 0.7873\n",
      "Epoch 55/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.3063 - accuracy: 0.8684 - val_loss: 0.5184 - val_accuracy: 0.7814\n",
      "Epoch 56/100\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 0.3037 - accuracy: 0.8669 - val_loss: 0.5366 - val_accuracy: 0.7716\n",
      "Epoch 57/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.3017 - accuracy: 0.8674 - val_loss: 0.5335 - val_accuracy: 0.7735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2994 - accuracy: 0.8725 - val_loss: 0.5148 - val_accuracy: 0.7863\n",
      "Epoch 59/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2966 - accuracy: 0.8725 - val_loss: 0.5181 - val_accuracy: 0.7814\n",
      "Epoch 60/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2955 - accuracy: 0.8694 - val_loss: 0.5130 - val_accuracy: 0.7912\n",
      "Epoch 61/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2927 - accuracy: 0.8757 - val_loss: 0.5240 - val_accuracy: 0.7794\n",
      "Epoch 62/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2926 - accuracy: 0.8706 - val_loss: 0.5220 - val_accuracy: 0.7814\n",
      "Epoch 63/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2899 - accuracy: 0.8730 - val_loss: 0.5240 - val_accuracy: 0.7794\n",
      "Epoch 64/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2869 - accuracy: 0.8745 - val_loss: 0.5254 - val_accuracy: 0.7843\n",
      "Epoch 65/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2862 - accuracy: 0.8733 - val_loss: 0.5511 - val_accuracy: 0.7725\n",
      "Epoch 66/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2850 - accuracy: 0.8718 - val_loss: 0.5358 - val_accuracy: 0.7824\n",
      "Epoch 67/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2832 - accuracy: 0.8792 - val_loss: 0.5349 - val_accuracy: 0.7784\n",
      "Epoch 68/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2790 - accuracy: 0.8789 - val_loss: 0.5355 - val_accuracy: 0.7863\n",
      "Epoch 69/100\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2772 - accuracy: 0.8811 - val_loss: 0.5307 - val_accuracy: 0.7833\n",
      "Epoch 70/100\n",
      "128/128 [==============================] - 2s 15ms/step - loss: 0.2746 - accuracy: 0.8836 - val_loss: 0.5447 - val_accuracy: 0.7735\n",
      "Epoch 71/100\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 0.2746 - accuracy: 0.8819 - val_loss: 0.5321 - val_accuracy: 0.7824\n",
      "Epoch 72/100\n",
      "128/128 [==============================] - 2s 16ms/step - loss: 0.2738 - accuracy: 0.8846 - val_loss: 0.5594 - val_accuracy: 0.7706\n",
      "Epoch 73/100\n",
      "128/128 [==============================] - 2s 14ms/step - loss: 0.2702 - accuracy: 0.8836 - val_loss: 0.5774 - val_accuracy: 0.7647\n",
      "Epoch 74/100\n",
      "128/128 [==============================] - 2s 14ms/step - loss: 0.2710 - accuracy: 0.8836 - val_loss: 0.5505 - val_accuracy: 0.7784\n",
      "Epoch 75/100\n",
      "128/128 [==============================] - 2s 13ms/step - loss: 0.2660 - accuracy: 0.8846 - val_loss: 0.5485 - val_accuracy: 0.7833\n",
      "Epoch 76/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.2623 - accuracy: 0.8868 - val_loss: 0.5467 - val_accuracy: 0.7824\n",
      "Epoch 77/100\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.2652 - accuracy: 0.8865 - val_loss: 0.5627 - val_accuracy: 0.7735\n",
      "Epoch 78/100\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.2624 - accuracy: 0.8897 - val_loss: 0.5590 - val_accuracy: 0.7775\n",
      "Epoch 79/100\n",
      "128/128 [==============================] - 2s 16ms/step - loss: 0.2572 - accuracy: 0.8897 - val_loss: 0.5584 - val_accuracy: 0.7784\n",
      "Epoch 80/100\n",
      "128/128 [==============================] - 2s 19ms/step - loss: 0.2553 - accuracy: 0.8877 - val_loss: 0.5572 - val_accuracy: 0.7853\n",
      "Epoch 81/100\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 0.2534 - accuracy: 0.8900 - val_loss: 0.5592 - val_accuracy: 0.7794\n",
      "Epoch 82/100\n",
      "128/128 [==============================] - 1s 12ms/step - loss: 0.2522 - accuracy: 0.8934 - val_loss: 0.5702 - val_accuracy: 0.7824\n",
      "Epoch 83/100\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.2521 - accuracy: 0.8892 - val_loss: 0.5630 - val_accuracy: 0.7824\n",
      "Epoch 84/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2510 - accuracy: 0.8892 - val_loss: 0.5694 - val_accuracy: 0.7784\n",
      "Epoch 85/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2475 - accuracy: 0.8963 - val_loss: 0.6037 - val_accuracy: 0.7618\n",
      "Epoch 86/100\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 0.2436 - accuracy: 0.8941 - val_loss: 0.5708 - val_accuracy: 0.7902\n",
      "Epoch 87/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2447 - accuracy: 0.8926 - val_loss: 0.5693 - val_accuracy: 0.7833\n",
      "Epoch 88/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2398 - accuracy: 0.8971 - val_loss: 0.5875 - val_accuracy: 0.7784\n",
      "Epoch 89/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2393 - accuracy: 0.8956 - val_loss: 0.5775 - val_accuracy: 0.7824\n",
      "Epoch 90/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2370 - accuracy: 0.9002 - val_loss: 0.5912 - val_accuracy: 0.7784\n",
      "Epoch 91/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2359 - accuracy: 0.8978 - val_loss: 0.5958 - val_accuracy: 0.7725\n",
      "Epoch 92/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2343 - accuracy: 0.9000 - val_loss: 0.5739 - val_accuracy: 0.7814\n",
      "Epoch 93/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2322 - accuracy: 0.9010 - val_loss: 0.5823 - val_accuracy: 0.7824\n",
      "Epoch 94/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2330 - accuracy: 0.9025 - val_loss: 0.5803 - val_accuracy: 0.7863\n",
      "Epoch 95/100\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.2281 - accuracy: 0.9042 - val_loss: 0.5817 - val_accuracy: 0.7824\n",
      "Epoch 96/100\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.2276 - accuracy: 0.9049 - val_loss: 0.6164 - val_accuracy: 0.7814\n",
      "Epoch 97/100\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 0.2260 - accuracy: 0.9012 - val_loss: 0.6088 - val_accuracy: 0.7745\n",
      "Epoch 98/100\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.2302 - accuracy: 0.9007 - val_loss: 0.6167 - val_accuracy: 0.7765\n",
      "Epoch 99/100\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.2232 - accuracy: 0.9037 - val_loss: 0.6131 - val_accuracy: 0.7755\n",
      "Epoch 100/100\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.2198 - accuracy: 0.9076 - val_loss: 0.5987 - val_accuracy: 0.7833\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, shuffle=True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of words covered in the embeddings = 0.7248732284488363\n",
      "WARNING:tensorflow:From <ipython-input-17-8b1ed815584a>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "processed, final_test = prepare_for_cnn(test)\n",
    "prediction = model.predict_classes(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7982491046557899"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test['target'], prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7530443253774962"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test['target'], prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
